# -*- coding: utf-8 -*-
"""manual-nn-regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGN6F-GDojPjzqCZ993cr1lk5jzbypE4

**Step 1: Setup and Data**
"""

import torch
import numpy as np
import matplotlib.pyplot as plt

# ReLU function
def relu(z):
    return torch.maximum(z, torch.tensor(0.0))

# ReLU gradient
def relu_grad(z):
    return (z > 0).float()

# Sample input X and target y (batch size = 5)
X = torch.tensor([[1.0, 2.0],
                  [2.0, 3.0],
                  [3.0, 1.0],
                  [0.0, 1.0],
                  [1.5, 2.5]], dtype=torch.float32)

y_true = torch.tensor([[10.0],
                       [15.0],
                       [13.0],
                       [5.0],
                       [17.0]], dtype=torch.float32)

torch.manual_seed(42)

# Initialize weights and biases
W1 = torch.randn((3, 2), dtype=torch.float32)
b1 = torch.randn((3,), dtype=torch.float32)

W2 = torch.randn((1, 3), dtype=torch.float32)
b2 = torch.randn((1,), dtype=torch.float32)

"""**Step 2: Initialize Weights & Biases**"""

torch.manual_seed(42)

# Initialize weights and biases
W1 = torch.randn((3, 2), dtype=torch.float32)
b1 = torch.randn((3,), dtype=torch.float32)

W2 = torch.randn((1, 3), dtype=torch.float32)
b2 = torch.randn((1,), dtype=torch.float32)

"""**Step 3: Training Loop (Manual Gradient Descent)**

we will run 100 epoches
"""

learning_rate = 0.01
loss_history = []

for epoch in range(100):
    ### ---- Forward Pass ---- ###
    Z1 = X @ W1.T + b1       # (5, 3)
    A1 = relu(Z1)            # (5, 3)
    y_pred = A1 @ W2.T + b2  # (5, 1)

    # Compute loss (MSE)
    error = y_pred - y_true
    loss = torch.mean(error ** 2)
    loss_history.append(loss.item())

    ### ---- Backward Pass ---- ###
    dL_dy_hat = 2 * error / X.shape[0]  # Mean squared derivative (5, 1)

    # Gradients for W2 and b2
    dL_dW2 = dL_dy_hat.T @ A1           # (1, 3)
    dL_db2 = torch.sum(dL_dy_hat, dim=0)  # (1,)

    # Hidden layer gradients
    dL_dA1 = dL_dy_hat @ W2             # (5, 3)
    dL_dZ1 = dL_dA1 * relu_grad(Z1)     # (5, 3)

    dL_dW1 = dL_dZ1.T @ X               # (3, 2)
    dL_db1 = torch.sum(dL_dZ1, dim=0)   # (3,)

    ### ---- Update Weights ---- ###
    W1 -= learning_rate * dL_dW1
    b1 -= learning_rate * dL_db1
    W2 -= learning_rate * dL_dW2
    b2 -= learning_rate * dL_db2

    # Print every 10 epochs
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

"""**ðŸ“ˆ Step 4: Plot Loss Curve**"""

plt.plot(loss_history)
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.grid(True)
plt.show()

